{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e4d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Natural Language Processing (NLP) is a subfield of Artificial Intelligence.\",\n",
    "    \"It helps machines understand and generate human language.\",\n",
    "    \"Common tasks include text classification, sentiment analysis, and machine translation.\",\n",
    "    \"Deep learning models like RNNs, LSTMs, and Transformers are widely used in NLP.\",\n",
    "    \"Tokenization is the process of splitting text into words or sentences.\",\n",
    "    \"Stemming reduces words to their root form, while lemmatization uses vocabulary rules.\",\n",
    "    \"Word embeddings like Word2Vec, GloVe, and FastText represent words as vectors.\",\n",
    "    \"Transformers such as BERT and GPT have revolutionized NLP tasks.\",\n",
    "    \"Named Entity Recognition (NER) identifies people, places, and organizations in text.\",\n",
    "    \"Part-of-Speech (POS) tagging labels words as nouns, verbs, adjectives, etc.\",\n",
    "    \"Sentiment analysis detects emotions or opinions expressed in text data.\",\n",
    "    \"Text summarization generates concise versions of long documents.\",\n",
    "    \"Machine translation converts text from one language to another automatically.\",\n",
    "    \"Question answering systems provide direct answers to user queries.\",\n",
    "    \"Chatbots use NLP to interact with humans in a conversational manner.\",\n",
    "    \"Speech recognition converts spoken language into text.\",\n",
    "    \"Language models predict the next word in a sequence of text.\",\n",
    "    \"Topic modeling discovers hidden themes within large text collections.\",\n",
    "    \"NLP is widely used in search engines, recommendation systems, and voice assistants.\",\n",
    "    \"Ethical challenges in NLP include bias, fairness, and misinformation detection.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eba7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What are the applications of Natural Language Processing in real life?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f147a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b625d6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'applications',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'in',\n",
       " 'real',\n",
       " 'life?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uq = user_query.lower().split(\" \")\n",
    "uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a77c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"The various applications are sentiment analysis and much more are there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc4e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = document.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb2e551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'are': 2,\n",
       "         'the': 1,\n",
       "         'various': 1,\n",
       "         'applications': 1,\n",
       "         'sentiment': 1,\n",
       "         'analysis': 1,\n",
       "         'and': 1,\n",
       "         'much': 1,\n",
       "         'more': 1,\n",
       "         'there': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uq_tokens = Counter(uq)\n",
    "doc_tokens = Counter(doc)\n",
    "doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7fd637f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'what': 1,\n",
       "         'are': 1,\n",
       "         'the': 1,\n",
       "         'applications': 1,\n",
       "         'of': 1,\n",
       "         'natural': 1,\n",
       "         'language': 1,\n",
       "         'processing': 1,\n",
       "         'in': 1,\n",
       "         'real': 1,\n",
       "         'life?': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uq_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c499edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "applications\n",
      "are\n"
     ]
    }
   ],
   "source": [
    "for tokens in uq_tokens.keys() & doc_tokens.keys():\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd8f3b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = []\n",
    "for tokens in uq_tokens.keys() & doc_tokens.keys():\n",
    "    mylist.append(uq_tokens[tokens]*doc_tokens[tokens])\n",
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afdad9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product = sum(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b85eec1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for tokens in uq_tokens.keys():\n",
    "    print(uq_tokens[tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b72f9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3166247903554"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_magnitude = math.sqrt(sum(math.pow(uq_tokens[tokens],2) for tokens in uq_tokens.keys()))\n",
    "query_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ec175e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.605551275463989"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_magnitude = math.sqrt(sum(math.pow(doc_tokens[tokens],2) for tokens in doc_tokens.keys()))\n",
    "doc_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36899a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3344968040028363"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = dot_product/(query_magnitude*doc_magnitude)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90e07f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"generate human language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9124ba18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6405126152203486"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosineSimilarity(query,document):\n",
    "    query =  query.lower().split(\" \")\n",
    "    document =  document.lower().split(\" \")\n",
    "    query_new = Counter(query)\n",
    "    document_new = Counter(document)\n",
    "    mylist = []\n",
    "    for tokens in query_new.keys() & document_new.keys():\n",
    "        mylist.append(query_new[tokens]*document_new[tokens])\n",
    "    dotProduct = sum(mylist)\n",
    "    qmagnitude = math.sqrt(sum(math.pow(query_new[tokens],2) for tokens in query_new.keys()))\n",
    "    dmagnitude = math.sqrt(sum(math.pow(document_new[tokens],2) for tokens in document_new.keys()))\n",
    "    similarity = dot_product/(qmagnitude*dmagnitude)\n",
    "    return similarity\n",
    "cosineSimilarity(user_query,document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50c03ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7302967433402214, 0.8164965809277259, 0.7302967433402214, 0.6405126152203486, 0.6963106238227914, 0.6666666666666667, 0.6963106238227914, 0.7302967433402214, 0.6963106238227914, 0.7302967433402214, 0.7302967433402214, 0.8164965809277259, 0.7302967433402214, 0.769800358919501, 0.6963106238227914, 0.8728715609439696, 0.6963106238227914, 0.769800358919501, 0.6666666666666667, 0.7302967433402214]\n",
      "Speech recognition converts spoken language into text.\n"
     ]
    }
   ],
   "source": [
    "def forAll(query,corpus):\n",
    "    simCorpus = []\n",
    "    for doc in corpus:\n",
    "        sim = cosineSimilarity(query,doc)\n",
    "        simCorpus.append(sim)\n",
    "    return simCorpus,corpus[simCorpus.index(max(simCorpus))]\n",
    "simCorp,abc = forAll(user_query,corpus)\n",
    "print(simCorp)\n",
    "print(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c91e83cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing (NLP) is a subfield of Artificial Intelligence.', 'It helps machines understand and generate human language.', 'Common tasks include text classification, sentiment analysis, and machine translation.', 'Deep learning models like RNNs, LSTMs, and Transformers are widely used in NLP.', 'Tokenization is the process of splitting text into words or sentences.', 'Stemming reduces words to their root form, while lemmatization uses vocabulary rules.', 'Word embeddings like Word2Vec, GloVe, and FastText represent words as vectors.', 'Transformers such as BERT and GPT have revolutionized NLP tasks.', 'Named Entity Recognition (NER) identifies people, places, and organizations in text.', 'Part-of-Speech (POS) tagging labels words as nouns, verbs, adjectives, etc.', 'Sentiment analysis detects emotions or opinions expressed in text data.', 'Text summarization generates concise versions of long documents.', 'Machine translation converts text from one language to another automatically.', 'Question answering systems provide direct answers to user queries.', 'Chatbots use NLP to interact with humans in a conversational manner.', 'Speech recognition converts spoken language into text.', 'Language models predict the next word in a sequence of text.', 'Topic modeling discovers hidden themes within large text collections.', 'NLP is widely used in search engines, recommendation systems, and voice assistants.', 'Ethical challenges in NLP include bias, fairness, and misinformation detection.']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85de496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2408a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore sentiment analysis applications. \n",
      "It offers diverse language processing tools.\n"
     ]
    }
   ],
   "source": [
    "full_response = []\n",
    "prompt = \"\"\"\n",
    "You are a bot that makes recommendations for activities. You answer in very short sentences about two lines and do not include extra information.\n",
    "This is the recommended activity: {document}\n",
    "The user input is: {user_query}\n",
    "Compile a recommendation to the user based on the recommended activity and the user input.\n",
    "\"\"\"\n",
    "\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"model\": \"gemma3:4b\",\n",
    "    \"prompt\": prompt.format(user_query=user_query, document=document)\n",
    "}\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    for line in response.iter_lines():\n",
    "        # filter out keep-alive new lines\n",
    "        if line:\n",
    "            decoded_line = json.loads(line.decode('utf-8'))\n",
    "            # print(decoded_line['response'])  # uncomment to results, token by token\n",
    "            full_response.append(decoded_line['response'])\n",
    "finally:\n",
    "    response.close()\n",
    "    \n",
    "    \n",
    "print(''.join(full_response))\n",
    "# print(full_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
